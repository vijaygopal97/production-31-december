# üéØ Comprehensive Load Balancing & Performance Analysis

## üìä Test Results Summary

**Test Date:** January 11, 2026  
**Test Duration:** 5 minutes (300 seconds)  
**Test Scale:** 50 QA, 50 CATI, 50 CAPI, 10 PM, 2 Admin

### Final Results

| User Type | Requests | Success Rate | Avg Response Time |
|-----------|----------|--------------|-------------------|
| Quality Agents | 1,350 | **18.81%** ‚ö†Ô∏è | 5.6s |
| CATI Interviewers | 500 | **0.00%** ‚ùå | 30s (timeout) |
| CAPI Interviewers | 5,300 | **100.00%** ‚úÖ | 1.96s |
| Project Managers | 540 | **100.00%** ‚úÖ | 4.86s |
| Company Admins | 120 | **100.00%** ‚úÖ | 4.72s |

**Overall:** 7,810 requests | 79.56% success rate

---

## üîç Why Load Balancing Didn't Work Efficiently

### Current Nginx Configuration

```nginx
upstream opine_backend {
    least_conn;  # ‚ùå PROBLEM: Least connections algorithm
    server 127.0.0.1:5000 max_fails=3 fail_timeout=30s weight=5;  # Primary (62.5%)
    server 172.31.47.152:5000 max_fails=3 fail_timeout=30s weight=3;  # Secondary (37.5%)
    keepalive 128;
}
```

### Root Causes of Load Balancing Failure

#### 1. **Algorithm Mismatch: `least_conn` vs Weight Distribution**

**The Problem:**
- You're using `least_conn` (least connections) algorithm
- But you also set weights (5:3)
- **`least_conn` IGNORES weights!** It only routes to the server with fewest active connections

**What Happened:**
- Quality Agent requests (heavy MongoDB aggregations) take 5-6 seconds each
- These requests hold connections for a long time
- Secondary server gets one request ‚Üí connection held for 6s ‚Üí `least_conn` sees it has fewer connections ‚Üí routes next request there
- This creates a "sticky" effect where heavy operations cluster on the secondary
- Secondary gets overwhelmed while primary sits underutilized

#### 2. **Secondary Server Spec/Resource Mismatch**

**Test Evidence:**
- Primary Server: Load 6.5, CPU 43%, Memory 35% ‚úÖ Healthy
- Secondary Server: Load 122+, CPU 95%, Memory 26% ‚ùå CRITICAL OVERLOAD

**The Issue:**
- Both servers have 8 VCPUs and 30GB RAM (same specs)
- But the secondary is handling disproportionately heavy operations
- Load average of 122 means 1,525% utilization of 8 cores = severe queuing

#### 3. **Request Type Distribution Problem**

**Heavy Operations (Failing):**
- Quality Agent assignment queries: Complex MongoDB aggregations
- CATI interview start: External API calls (DeepCall) + MongoDB queries
- These operations take 5-30 seconds

**Light Operations (Succeeding):**
- CAPI interview start: Simple MongoDB queries (1.96s avg)
- Project Manager reports: Read-heavy queries (4.86s avg)
- Company Admin reports: Read-heavy queries (4.72s avg)

**The Problem:**
- `least_conn` doesn't distinguish between heavy and light operations
- All requests are treated equally
- Heavy operations cluster on one server, causing overload

#### 4. **No Health Checks or Circuit Breaker**

**What's Missing:**
- No health check endpoint monitoring
- No automatic failover when server is overloaded
- `max_fails=3` only triggers on connection failures, not on slow responses
- Secondary server was overloaded but still receiving requests

---

## üîç Bottleneck Analysis

### Primary Bottleneck: Secondary Server Overload

**Evidence:**
- Secondary load: 122+ (should be < 8)
- CPU: 95%+
- Quality Agents: 18.81% success (1,096 failures)
- CATI: 0% success (500 failures, all timeouts)

**Root Cause:**
1. Heavy MongoDB aggregation queries in Quality Agent assignment endpoint
2. External API calls (DeepCall) in CATI interview start
3. `least_conn` algorithm routing heavy operations to secondary
4. No request type segregation

### Secondary Bottleneck: MongoDB Aggregation Performance

**Evidence:**
- Quality Agent requests: 5.6s average response time
- P95 response time: 30s
- P99 response time: 30s (timeouts)

**Root Cause:**
- Quality Agent "next review assignment" query likely does:
  - Complex aggregation pipeline
  - Multiple $lookup operations
  - Filtering by AC assignment
  - Sorting and limiting
- No caching of frequently accessed data
- No indexing optimization visible

### Tertiary Bottleneck: External API Dependency (CATI)

**Evidence:**
- CATI: 0% success, 30s average (all timeouts)
- All 500 requests failed

**Root Cause:**
- CATI interview start calls DeepCall API (`https://s-ct3.sarv.com/v2/clickToCall/para`)
- External API has timeout: 30s
- When secondary is overloaded, Node.js event loop is blocked
- External API calls queue up and timeout

---

## üèÜ How Top-Tier Companies (WhatsApp, Amazon) Handle This

### 1. **Route-Based Load Balancing**

**WhatsApp/Amazon Approach:**
- Different endpoints route to different server pools
- Heavy operations ‚Üí Dedicated heavy-operation servers
- Light operations ‚Üí Light-operation servers
- Read operations ‚Üí Read replicas

**Implementation:**
```nginx
# Light operations (CAPI, reports)
upstream opine_backend_light {
    least_conn;
    server 127.0.0.1:5000 weight=5;
    server 172.31.47.152:5000 weight=3;
    keepalive 128;
}

# Heavy operations (Quality Agents, CATI)
upstream opine_backend_heavy {
    least_conn;
    server 127.0.0.1:5000 weight=8;  # Primary handles heavy ops
    server 172.31.47.152:5000 weight=1 backup;  # Secondary as backup only
    keepalive 64;
}

# Route based on path
location /api/survey-responses/next-review {
    proxy_pass http://opine_backend_heavy;
}

location /api/cati-interview/start {
    proxy_pass http://opine_backend_heavy;
}

location /api {
    proxy_pass http://opine_backend_light;
}
```

### 2. **Caching Layer (Redis)**

**WhatsApp/Amazon Approach:**
- Cache frequently accessed data (survey responses, assignments)
- Cache aggregation results
- Invalidate cache on updates

**Implementation:**
- Redis cache for Quality Agent assignments
- Cache assignment queue for 30-60 seconds
- Reduce MongoDB aggregation load by 80-90%

**Benefits:**
- Quality Agent requests: 5.6s ‚Üí 50-200ms (95% reduction)
- MongoDB load: Reduced by 80-90%
- Secondary server load: Reduced significantly

### 3. **Database Query Optimization**

**WhatsApp/Amazon Approach:**
- Optimize aggregation pipelines
- Add proper indexes
- Use read preferences (read from secondary for reports)
- Separate read/write operations

**Implementation:**
- Index on `survey`, `status`, `selectedAC`, `reviewAssignment`
- Compound indexes for common query patterns
- Read preference: `secondaryPreferred` for reports
- Read preference: `primary` for assignments (consistency)

### 4. **Async Processing for External APIs**

**WhatsApp/Amazon Approach:**
- External API calls ‚Üí Message queue (RabbitMQ, SQS)
- Background workers process queue
- Return immediately with job ID
- Poll for status or use WebSocket

**Implementation:**
- CATI interview start ‚Üí Queue job ‚Üí Return job ID immediately
- Background worker processes DeepCall API
- WebSocket/SSE for status updates
- Response time: 30s ‚Üí 100ms (300x improvement)

### 5. **Horizontal Scaling with Auto-Scaling**

**WhatsApp/Amazon Approach:**
- Auto-scale based on CPU/memory/request queue
- Add servers dynamically during peak
- Remove servers during low traffic

**Implementation:**
- AWS Auto Scaling Groups
- Scale based on CPU > 70% or request queue > 100
- Add 1 server at a time, max 10 servers
- Health checks prevent adding unhealthy servers

### 6. **Connection Pooling & Keep-Alive**

**WhatsApp/Amazon Approach:**
- Optimize connection pools
- Reuse connections
- Connection per worker process

**Current:** Good (keepalive 128) ‚úÖ

### 7. **Request Rate Limiting & Circuit Breaker**

**WhatsApp/Amazon Approach:**
- Rate limit per user/IP
- Circuit breaker for failing services
- Automatic failover

**Implementation:**
- Rate limit: 100 requests/minute per user
- Circuit breaker: Open after 5 failures in 10s
- Fallback to cached data or error response

### 8. **Monitoring & Alerting**

**WhatsApp/Amazon Approach:**
- Real-time monitoring (Prometheus, CloudWatch)
- Alerts for anomalies
- Automatic remediation

**Implementation:**
- Monitor: CPU, memory, response times, error rates
- Alert: CPU > 80%, response time > 5s, error rate > 5%
- Auto-scaling triggers on alerts

---

## üí° Recommended Solutions (Without Changing Functionality)

### **Phase 1: Quick Wins (1-2 Days)**

#### 1. **Fix Load Balancing Algorithm**

**Change:** Use `weighted least_conn` or `ip_hash` for heavy operations

```nginx
upstream opine_backend {
    # Option A: Weighted round-robin (respects weights)
    # least_conn;  # REMOVE THIS
    server 127.0.0.1:5000 weight=8;  # Primary: 80%
    server 172.31.47.152:5000 weight=2;  # Secondary: 20%
    keepalive 128;
}

# OR Option B: Route heavy ops to primary only
upstream opine_backend_heavy {
    server 127.0.0.1:5000;
    server 172.31.47.152:5000 backup;  # Only if primary fails
}
```

**Impact:** Secondary load: 122+ ‚Üí 20-30 (80% reduction)

#### 2. **Route Heavy Operations to Primary**

**Change:** Separate upstreams for heavy vs light operations

```nginx
# Heavy operations (Quality Agents, CATI)
upstream opine_backend_heavy {
    server 127.0.0.1:5000;
}

# Light operations (CAPI, Reports)
upstream opine_backend_light {
    least_conn;
    server 127.0.0.1:5000 weight=5;
    server 172.31.47.152:5000 weight=3;
}
```

**Impact:** Quality Agent success: 18.81% ‚Üí 60-70%

#### 3. **Add Request Timeout**

**Change:** Reduce timeout for heavy operations

```nginx
location /api/survey-responses/next-review {
    proxy_pass http://opine_backend_heavy;
    proxy_connect_timeout 5s;
    proxy_send_timeout 10s;
    proxy_read_timeout 10s;
}
```

**Impact:** Prevents requests from hanging, faster failure detection

---

### **Phase 2: Medium-Term (1-2 Weeks)**

#### 4. **Add Redis Caching**

**Implementation:**
- Cache Quality Agent assignment queue (30s TTL)
- Cache survey metadata (5min TTL)
- Cache report aggregations (1min TTL)

**Impact:**
- Quality Agent response time: 5.6s ‚Üí 100ms (98% reduction)
- MongoDB load: Reduced by 80%
- Secondary load: Further reduced

#### 5. **Database Index Optimization**

**Implementation:**
- Compound index: `{survey: 1, status: 1, selectedAC: 1, reviewAssignment: 1}`
- Index on: `{survey: 1, interviewMode: 1, status: 1}`
- Analyze slow queries, add missing indexes

**Impact:**
- Query time: 5.6s ‚Üí 500ms (90% reduction)

#### 6. **Async Processing for CATI**

**Implementation:**
- CATI start ‚Üí Queue job ‚Üí Return job ID
- Background worker ‚Üí DeepCall API
- WebSocket/SSE for status

**Impact:**
- CATI response time: 30s ‚Üí 100ms (300x improvement)
- Success rate: 0% ‚Üí 95%+

---

### **Phase 3: Long-Term (1-3 Months)**

#### 7. **Horizontal Scaling**

**Implementation:**
- Add 3rd server
- Auto-scaling based on load
- Load balancer (ELB/ALB) with health checks

**Impact:**
- Capacity: 2x-3x increase
- Reliability: No single point of failure

#### 8. **Read Replicas for Reports**

**Implementation:**
- MongoDB read preference: `secondaryPreferred` for reports
- Read preference: `primary` for writes/assignments

**Impact:**
- Primary MongoDB load: Reduced by 40%
- Report response time: Improved by 20%

#### 9. **Microservices Architecture**

**Implementation:**
- Separate service for Quality Agent assignments
- Separate service for CATI processing
- Separate service for reports

**Impact:**
- Independent scaling
- Better resource utilization
- Fault isolation

---

## üìã Priority Recommendations (Top-Down)

### **Critical (Do First)**

1. **Fix Load Balancing Algorithm** ‚ö° (1 hour)
   - Change `least_conn` to weighted round-robin OR
   - Route heavy ops to primary only
   - Impact: Immediate 80% reduction in secondary load

2. **Route Heavy Operations to Primary** ‚ö° (2 hours)
   - Separate upstreams for heavy vs light
   - Route Quality Agents & CATI to primary
   - Impact: Quality Agent success: 18.81% ‚Üí 60-70%

### **High Priority (Do Next Week)**

3. **Add Redis Caching** üî• (2-3 days)
   - Cache Quality Agent assignments
   - Cache survey metadata
   - Impact: Response time: 5.6s ‚Üí 100ms (98% improvement)

4. **Database Index Optimization** üî• (1 day)
   - Analyze slow queries
   - Add compound indexes
   - Impact: Query time: 5.6s ‚Üí 500ms (90% improvement)

5. **Async Processing for CATI** üî• (3-4 days)
   - Queue-based architecture
   - Background workers
   - Impact: CATI success: 0% ‚Üí 95%+

### **Medium Priority (Next Month)**

6. **Add Health Checks & Circuit Breaker** (2-3 days)
7. **Horizontal Scaling (3rd Server)** (1 week)
8. **Read Replicas for Reports** (1 week)
9. **Monitoring & Alerting** (3-4 days)

---

## üéØ Expected Performance After Fixes

### **After Phase 1 (Quick Wins)**

| User Type | Current | After Phase 1 | Improvement |
|-----------|---------|---------------|-------------|
| Quality Agents | 18.81% | 60-70% | +300% |
| CATI Interviewers | 0% | 20-30% | +‚àû |
| Secondary Server Load | 122+ | 20-30 | -80% |

### **After Phase 2 (Medium-Term)**

| User Type | After Phase 1 | After Phase 2 | Improvement |
|-----------|---------------|---------------|-------------|
| Quality Agents | 60-70% | 95%+ | +50% |
| CATI Interviewers | 20-30% | 95%+ | +300% |
| Quality Agent Response Time | 5.6s | 100-500ms | -95% |
| CATI Response Time | 30s | 100ms | -99% |

### **After Phase 3 (Long-Term)**

- **Capacity:** Handle 10x current load
- **Reliability:** 99.9% uptime
- **Response Time:** < 1s for all operations
- **Auto-Scaling:** Handle traffic spikes automatically

---

## üìù Summary

### **Main Problems:**

1. **Load Balancing Algorithm:** `least_conn` doesn't respect weights and causes heavy operations to cluster
2. **No Request Type Segregation:** Heavy and light operations use same pool
3. **No Caching:** MongoDB aggregations run on every request
4. **External API Blocking:** CATI calls block Node.js event loop
5. **No Async Processing:** Synchronous external API calls cause timeouts

### **Main Solutions:**

1. **Fix load balancing:** Use weighted round-robin OR route heavy ops to primary
2. **Add Redis caching:** 98% response time reduction
3. **Database optimization:** 90% query time reduction
4. **Async processing:** 300x improvement for CATI
5. **Horizontal scaling:** 2-3x capacity increase

### **Top-Tier Company Approach:**

- Route-based load balancing
- Multi-layer caching (Redis, CDN)
- Database query optimization
- Async processing for external APIs
- Horizontal scaling with auto-scaling
- Comprehensive monitoring

**Estimated Total Improvement:**
- Quality Agent success: 18.81% ‚Üí 95%+ (5x improvement)
- CATI success: 0% ‚Üí 95%+ (‚àû improvement)
- Response times: 5.6s ‚Üí 100ms (56x improvement)
- Capacity: 2-3x increase
- Cost efficiency: 50% better resource utilization





